{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa1482eb",
      "metadata": {
        "id": "fa1482eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f67jGL23fQbo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67jGL23fQbo",
        "outputId": "77ddb833-5b22-40be-a578-705114f4a45a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Set dataset path accordingly\n",
        "if IN_COLAB:\n",
        "    ! git clone https://github.com/MrKiwix/IAPR-project.git\n",
        "    %cd IAPR-project\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT_DIR = Path('/content/drive/MyDrive/IAPR')\n",
        "else:\n",
        "    ROOT_DIR = Path('./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fcde645f",
      "metadata": {
        "id": "fcde645f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from skimage import io, transform\n",
        "from src.helper import display_sample\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "15a0e5e4",
      "metadata": {
        "id": "15a0e5e4"
      },
      "outputs": [],
      "source": [
        "class ChocolateDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, label_csv, transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.label_df = pd.read_csv(label_csv)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist\n",
        "\n",
        "        img_path = Path(f\"{self.data_dir}/L{self.label_df.iloc[idx, 0]}.JPG\")\n",
        "\n",
        "        image = io.imread(img_path)\n",
        "        label = self.label_df.iloc[idx, 1:]\n",
        "        label = label.astype(int)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class LabelToTensor:\n",
        "    def __call__(self, label):\n",
        "        return torch.tensor(label.to_numpy())\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        # first conv layer, downsampling if stride > 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CountHead(nn.Module):\n",
        "    \"\"\"\n",
        "    in_channels : #channels coming from the encoder\n",
        "    hidden      : size of the intermediate layer (default 512)\n",
        "    n_classes   : how many categories we count\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=512, hidden=512, n_classes=3, p_drop=0.2):\n",
        "        super().__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)           # (B, C, H, W) → (B, C, 1, 1)\n",
        "\n",
        "        self.regressor = nn.Sequential(              # (B, C) → (B, n_classes)\n",
        "            nn.Flatten(1),                           # (B, C, 1, 1) → (B, C)\n",
        "            nn.Linear(in_channels, hidden, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(hidden, n_classes, bias=True)  # final counts (float)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gap(x)\n",
        "        return self.regressor(x)                     # shape (B, n_classes)\n",
        "\n",
        "\n",
        "class ChocoNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "\n",
        "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(ResBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(ResBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, 512, 2, stride=2)\n",
        "\n",
        "        self.head = CountHead(in_channels=512, n_classes=13)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            batch_size = X[0]\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1bf13173",
      "metadata": {
        "id": "1bf13173"
      },
      "outputs": [],
      "source": [
        "# -----------------  PREPARE THE DATA  -----------------\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "SYNTHETIC = True\n",
        "\n",
        "NUM_CLASSES = 13\n",
        "IMG_SIZE    = (240, 160)          # height, width  (change as you like)\n",
        "\n",
        "if SYNTHETIC:\n",
        "    label_csv  = ROOT_DIR / Path(\"./data/synthetic_train.csv\")\n",
        "    images_dir = ROOT_DIR / Path(\"./data/synthetic_train\")\n",
        "else:\n",
        "    label_csv  = ROOT_DIR / Path(\"./data/train.csv\")\n",
        "    images_dir = ROOT_DIR / Path(\"./data/train\")\n",
        "\n",
        "# 1) TRAIN transforms (with augmentation)\n",
        "train_tf = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True), \n",
        "    v2.Resize(IMG_SIZE, antialias=True),                \n",
        "    # v2.RandomHorizontalFlip(0.5),\n",
        "    # v2.RandomRotation(15),\n",
        "    # v2.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    v2.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
        "])\n",
        "\n",
        "# 2) TEST/VALID transforms (no augmentation)\n",
        "test_tf = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Resize(IMG_SIZE, antialias=True),\n",
        "    v2.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
        "])\n",
        "\n",
        "\"\"\"dataset = ChocolateDataset(\n",
        "    data_dir=images_dir,\n",
        "    label_csv=label_csv,\n",
        "    transform=img_tf,\n",
        "    target_transform=\n",
        "    \n",
        "    \n",
        "    or()\n",
        ")\n",
        "\n",
        "# split 80 % / 20 %\n",
        "train_len = int(0.8 * len(dataset))\n",
        "test_len  = len(dataset) - train_len\n",
        "train_ds, test_ds = random_split(dataset, [train_len, test_len],\n",
        "                                 generator=torch.Generator().manual_seed(42))\"\"\"\n",
        "\n",
        "# Full dataset\n",
        "full_dataset = ChocolateDataset(\n",
        "    data_dir=images_dir,\n",
        "    label_csv=label_csv,\n",
        "    transform=None,  # temp placeholder\n",
        "    target_transform=LabelToTensor()\n",
        ")\n",
        "\n",
        "# Split indexes\n",
        "train_len = int(0.8 * len(full_dataset))\n",
        "test_len  = len(full_dataset) - train_len\n",
        "train_idxs, test_idxs = torch.utils.data.random_split(\n",
        "    range(len(full_dataset)), [train_len, test_len], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# Subset with transforms applied\n",
        "train_ds = torch.utils.data.Subset(\n",
        "    ChocolateDataset(images_dir, label_csv, transform=train_tf, target_transform=LabelToTensor()),\n",
        "    train_idxs)\n",
        "\n",
        "test_ds = torch.utils.data.Subset(\n",
        "    ChocolateDataset(images_dir, label_csv, transform=test_tf, target_transform=LabelToTensor()),\n",
        "    test_idxs)\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 0\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size,\n",
        "                          shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size,\n",
        "                          shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "23a4a346",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23a4a346",
        "outputId": "f13792b5-7c83-4ca6-c533-039f3846f4c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# -----------------  BUILD MODEL  -----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = ChocoNetwork().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2a6d3969",
      "metadata": {
        "id": "2a6d3969"
      },
      "outputs": [],
      "source": [
        "# -----------------  OPTIMISER & SCHEDULER  -----------------\n",
        "\n",
        "# two parameter groups: head LR 1e-3, backbone LR 1e-4 (when unfrozen)\n",
        "optim_groups = [\n",
        "    {\"params\": [p for n, p in model.named_parameters() if n.startswith(\"head.\")],\n",
        "     \"lr\": 1e-3},\n",
        "    {\"params\": [p for n, p in model.named_parameters() if not n.startswith(\"head.\")],\n",
        "     \"lr\": 1e-4},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(optim_groups, weight_decay=1e-4)\n",
        "\n",
        "# Smooth L1 (Huber) with β=1.0\n",
        "criterion = nn.SmoothL1Loss(beta=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "447d2386",
      "metadata": {
        "id": "447d2386"
      },
      "outputs": [],
      "source": [
        "# -----------------  TRAIN / EVAL LOOPS  -----------------\n",
        "def train_epoch(loader, net, loss_fn, optim, epoch):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, targets in loader:\n",
        "        imgs     = imgs.to(device, non_blocking=True)\n",
        "        targets  = targets.float().to(device, non_blocking=True)\n",
        "\n",
        "        preds = net(imgs)\n",
        "        loss  = loss_fn(preds, targets)\n",
        "\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, net, loss_fn):\n",
        "    net.eval()\n",
        "    total_loss = 0.0\n",
        "    mae_sum    = torch.zeros(NUM_CLASSES, device=device)\n",
        "\n",
        "    for imgs, targets in loader:\n",
        "        imgs    = imgs.to(device, non_blocking=True)\n",
        "        targets = targets.float().to(device, non_blocking=True)\n",
        "\n",
        "        preds = net(imgs)\n",
        "        total_loss += loss_fn(preds, targets).item() * imgs.size(0)\n",
        "\n",
        "        mae_sum += (preds - targets).abs().sum(dim=0)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    mae      = (mae_sum / len(loader.dataset)).cpu()   # per-class MAE\n",
        "\n",
        "    return avg_loss, mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cd194c8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd194c8d",
        "outputId": "c4f64a7b-f293-4cde-99df-4506622fd24e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/60 [05:30<5:25:18, 330.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | train loss: 0.2759 | val loss: 0.3166 | val MAE/class: [0.50, 0.46, 0.45, 0.51, 0.53, 0.49, 0.48, 0.47, 0.53, 0.49, 0.60, 0.56, 0.65]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 2/60 [11:05<5:21:47, 332.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | train loss: 0.2514 | val loss: 0.2603 | val MAE/class: [0.52, 0.50, 0.50, 0.57, 0.56, 0.53, 0.52, 0.50, 0.52, 0.55, 0.59, 0.58, 0.58]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 3/60 [16:40<5:17:23, 334.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | train loss: 0.2446 | val loss: 0.2893 | val MAE/class: [0.57, 0.64, 0.58, 0.56, 0.53, 0.61, 0.72, 0.59, 0.64, 0.67, 0.82, 0.76, 0.57]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 3/60 [21:34<6:49:59, 431.58s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m best_val_loss    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     val_loss, val_mae \u001b[38;5;241m=\u001b[39m eval_epoch(test_loader, model, criterion)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# ---- logging ----\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[7], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(loader, net, loss_fn, optim, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     14\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 16\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -----------------  TRAINING DRIVER  -----------------\n",
        "EPOCHS           = 60\n",
        "best_val_loss    = float(\"inf\")\n",
        "\n",
        "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
        "\n",
        "    train_loss = train_epoch(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_loss, val_mae = eval_epoch(test_loader, model, criterion)\n",
        "\n",
        "    # ---- logging ----\n",
        "    mae_str = \", \".join([f\"{m:.2f}\" for m in val_mae])\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss: {train_loss:.4f} | \"\n",
        "          f\"val loss: {val_loss:.4f} | \"\n",
        "          f\"val MAE/class: [{mae_str}]\")\n",
        "\n",
        "    # save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_choco_count.pt\")\n",
        "\n",
        "print(\"Training complete.  Best val loss:\", best_val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ndM9O5ucbt_7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndM9O5ucbt_7",
        "outputId": "a65679dc-c495-4e6b-d855-0c10860a047f"
      },
      "outputs": [],
      "source": [
        "# -----------------  TESTING  -----------------\n",
        "\n",
        "class ChocolateTestDataset(Dataset):\n",
        "    \"\"\"Dataset for test images where we don't have labels\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Get all image files from directory\n",
        "        self.image_files = [f for f in os.listdir(data_dir) \n",
        "                           if f.endswith('.JPG') or f.endswith('.jpg')]\n",
        "        # Sort to ensure consistent ordering\n",
        "        self.image_files.sort()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = Path(f\"{self.data_dir}/{img_name}\")\n",
        "        \n",
        "        # Extract image ID from filename (assuming format like \"L123.JPG\")\n",
        "        image_id = img_name.split('.')[0]\n",
        "        if image_id.startswith('L'):\n",
        "            image_id = image_id[1:]  # Remove 'L' prefix if present\n",
        "            \n",
        "        image = io.imread(img_path)\n",
        "        \n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        # Return image and its ID\n",
        "        return image, image_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e20dd8c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def predict_and_save(model, test_dataset, output_csv, device, class_names, batch_size=32, num_workers=0):\n",
        "    \"\"\"\n",
        "    Run predictions on test dataset and save results to CSV\n",
        "    \n",
        "    Args:\n",
        "        model: Trained PyTorch model\n",
        "        test_dataset: Dataset of test images\n",
        "        output_csv: Path to save predictions\n",
        "        device: Device to run model on (cuda/cpu)\n",
        "        batch_size: Batch size for DataLoader\n",
        "        num_workers: Number of workers for DataLoader\n",
        "    \"\"\"\n",
        "    # Create DataLoader\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, \n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,  # Important to keep order for matching with IDs\n",
        "        num_workers=num_workers, \n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Lists to store predictions and image IDs\n",
        "    all_preds = []\n",
        "    all_ids = []\n",
        "    \n",
        "    # Run inference\n",
        "    print(f\"Running predictions on {len(test_dataset)} images...\")\n",
        "    with torch.no_grad():\n",
        "        for images, img_ids in tqdm(test_loader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            preds = model(images)\n",
        "            \n",
        "            # Convert predictions to integers (since we're counting chocolates)\n",
        "            rounded_preds = torch.round(preds).int()\n",
        "            \n",
        "            # Add batch results to lists\n",
        "            all_preds.extend(rounded_preds.cpu().numpy())\n",
        "            all_ids.extend(img_ids)\n",
        "    \n",
        "    # Create DataFrame from predictions\n",
        "    print(\"Creating CSV output...\")\n",
        "    df = pd.DataFrame(all_preds)\n",
        "    \n",
        "    # Rename columns to match expected format (e.g., class_1, class_2, etc.)\n",
        "    df.columns = class_names\n",
        "    \n",
        "    # Add image IDs as first column\n",
        "    df.insert(0, 'id', all_ids)\n",
        "    \n",
        "    # Save to CSV\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Predictions saved to {output_csv}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b9e33b8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running predictions on 180 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [01:13<00:00, 12.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating CSV output...\n",
            "Predictions saved to data\\predictions.csv\n",
            "\n",
            "Sample predictions:\n",
            "        id  Jelly White  Jelly Milk  Jelly Black  Amandina  Crème brulée  \\\n",
            "0  1000757            0           1            1         0             0   \n",
            "1  1000758            0           0            0         1             1   \n",
            "2  1000759            0           1            0         0             1   \n",
            "3  1000760            0           0            0         0             0   \n",
            "4  1000761            0           0            0         0             0   \n",
            "\n",
            "   Triangolo  Tentation noir  Comtesse  Noblesse  Noir authentique  \\\n",
            "0          0               0         0         0                 0   \n",
            "1          0               0         1         0                 0   \n",
            "2          0               2         0         0                 1   \n",
            "3          0               0         0         0                 1   \n",
            "4          0               0         1         0                 0   \n",
            "\n",
            "   Passion au lait  Arabia  Stracciatella  \n",
            "0                1       1              0  \n",
            "1                0       0              0  \n",
            "2                0       0              0  \n",
            "3                1       0              0  \n",
            "4                0       2              0  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TEST_DIR = Path(\"./data/test\")\n",
        "OUTPUT_CSV = Path(\"./data/predictions.csv\")\n",
        "test_dataset = ChocolateTestDataset(\n",
        "    data_dir=TEST_DIR,\n",
        "    transform=test_tf\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "model = ChocoNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model/best_choco_count.pt\"))\n",
        "\n",
        "# Generate predictions and save to CSV\n",
        "predictions_df = predict_and_save(\n",
        "    model=model,\n",
        "    test_dataset=test_dataset,\n",
        "    output_csv=OUTPUT_CSV,\n",
        "    device=device,\n",
        "    class_names=[\"Jelly White\",\"Jelly Milk\",\"Jelly Black\",\"Amandina\",\"Crème brulée\",\"Triangolo\",\"Tentation noir\",\"Comtesse\",\"Noblesse\",\"Noir authentique\",\"Passion au lait\",\"Arabia\",\"Stracciatella\"],\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Display first few predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "print(predictions_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f04f53b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 11438157\n"
          ]
        }
      ],
      "source": [
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0975e8f5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl-cuda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
