{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa1482eb",
      "metadata": {
        "id": "fa1482eb"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Check if running in Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# Set dataset path accordingly\n",
        "if IN_COLAB:\n",
        "    ! git clone https://github.com/MrKiwix/IAPR-project.git\n",
        "    %cd IAPR-project\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT_DIR = Path('/content/drive/MyDrive')\n",
        "else:\n",
        "    ROOT_DIR = Path('./')"
      ],
      "metadata": {
        "id": "f67jGL23fQbo",
        "outputId": "77ddb833-5b22-40be-a578-705114f4a45a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f67jGL23fQbo",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IAPR-project'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 20 (delta 2), reused 16 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (20/20), 91.96 KiB | 509.00 KiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/IAPR-project\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fcde645f",
      "metadata": {
        "id": "fcde645f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision.transforms import v2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from skimage import io, transform\n",
        "from src.helper import display_sample\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "15a0e5e4",
      "metadata": {
        "id": "15a0e5e4"
      },
      "outputs": [],
      "source": [
        "class ChocolateDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, label_csv, transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.label_df = pd.read_csv(label_csv)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist\n",
        "\n",
        "        img_path = Path(f\"{self.data_dir}/L{self.label_df.iloc[idx, 0]}.JPG\")\n",
        "\n",
        "        image = io.imread(img_path)\n",
        "        label = self.label_df.iloc[idx, 1:]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class LabelToTensor:\n",
        "    def __call__(self, label):\n",
        "        return torch.tensor(label.to_numpy())\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        # first conv layer, downsampling if stride > 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CountHead(nn.Module):\n",
        "    \"\"\"\n",
        "    in_channels : #channels coming from the encoder\n",
        "    hidden      : size of the intermediate layer (default 512)\n",
        "    n_classes   : how many categories we count\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=512, hidden=512, n_classes=3, p_drop=0.2):\n",
        "        super().__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)           # (B, C, H, W) → (B, C, 1, 1)\n",
        "\n",
        "        self.regressor = nn.Sequential(              # (B, C) → (B, n_classes)\n",
        "            nn.Flatten(1),                           # (B, C, 1, 1) → (B, C)\n",
        "            nn.Linear(in_channels, hidden, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p_drop),\n",
        "            nn.Linear(hidden, n_classes, bias=True)  # final counts (float)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gap(x)\n",
        "        return self.regressor(x)                     # shape (B, n_classes)\n",
        "\n",
        "\n",
        "class ChocoNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "\n",
        "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(ResBlock, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(ResBlock, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, 512, 2, stride=2)\n",
        "\n",
        "        self.head = CountHead(in_channels=512, n_classes=13)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1bf13173",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bf13173",
        "outputId": "f7590b71-7f7b-4696-d3c7-53b1e0716bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# -----------------  PREPARE THE DATA  -----------------\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "NUM_CLASSES = 13\n",
        "IMG_SIZE    = (120, 180)          # height, width  (change as you like)\n",
        "\n",
        "label_csv  = ROOT_DIR / Path(\"./data/train.csv\")\n",
        "images_dir = ROOT_DIR / Path(\"./data/train\")\n",
        "\n",
        "# transforms: uint8 [0-255] -> float32 [0-1]  + simple resize\n",
        "img_tf = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Resize(IMG_SIZE, antialias=True),\n",
        "])\n",
        "\n",
        "dataset = ChocolateDataset(\n",
        "    data_dir=images_dir,\n",
        "    label_csv=label_csv,\n",
        "    transform=img_tf,\n",
        "    target_transform=LabelToTensor()\n",
        ")\n",
        "\n",
        "# split 80 % / 20 %\n",
        "train_len = int(0.8 * len(dataset))\n",
        "test_len  = len(dataset) - train_len\n",
        "train_ds, test_ds = random_split(dataset, [train_len, test_len],\n",
        "                                 generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "batch_size = 32           # ↑ from 4  (better gradient statistics)\n",
        "num_workers = 4           # set 0 on Windows if you hit issues\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size,\n",
        "                          shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size,\n",
        "                          shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "23a4a346",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23a4a346",
        "outputId": "f13792b5-7c83-4ca6-c533-039f3846f4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# -----------------  BUILD MODEL  -----------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = ChocoNetwork().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2a6d3969",
      "metadata": {
        "id": "2a6d3969"
      },
      "outputs": [],
      "source": [
        "# -----------------  OPTIMISER & SCHEDULER  -----------------\n",
        "# freeze backbone for 5 epochs\n",
        "for name, p in model.named_parameters():\n",
        "    if not name.startswith(\"head.\"):\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "# two parameter groups: head LR 1e-3, backbone LR 1e-4 (when unfrozen)\n",
        "optim_groups = [\n",
        "    {\"params\": [p for n, p in model.named_parameters() if n.startswith(\"head.\")],\n",
        "     \"lr\": 1e-3},\n",
        "    {\"params\": [p for n, p in model.named_parameters() if not n.startswith(\"head.\")],\n",
        "     \"lr\": 1e-4},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(optim_groups, weight_decay=1e-4)\n",
        "\n",
        "# Smooth L1 (Huber) with β=1.0\n",
        "criterion = nn.SmoothL1Loss(beta=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "447d2386",
      "metadata": {
        "id": "447d2386"
      },
      "outputs": [],
      "source": [
        "# -----------------  TRAIN / EVAL LOOPS  -----------------\n",
        "def train_epoch(loader, net, loss_fn, optim, epoch):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, targets in loader:\n",
        "        imgs     = imgs.to(device, non_blocking=True)\n",
        "        targets  = targets.float().to(device, non_blocking=True)\n",
        "\n",
        "        preds = net(imgs)\n",
        "        loss  = loss_fn(preds, targets)\n",
        "\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(loader, net, loss_fn):\n",
        "    net.eval()\n",
        "    total_loss = 0.0\n",
        "    mae_sum    = torch.zeros(NUM_CLASSES, device=device)\n",
        "\n",
        "    for imgs, targets in loader:\n",
        "        imgs    = imgs.to(device, non_blocking=True)\n",
        "        targets = targets.float().to(device, non_blocking=True)\n",
        "\n",
        "        preds = net(imgs)\n",
        "        total_loss += loss_fn(preds, targets).item() * imgs.size(0)\n",
        "\n",
        "        mae_sum += (preds - targets).abs().sum(dim=0)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    mae      = (mae_sum / len(loader.dataset)).cpu()   # per-class MAE\n",
        "\n",
        "    return avg_loss, mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd194c8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd194c8d",
        "outputId": "c4f64a7b-f293-4cde-99df-4506622fd24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train loss: 0.3480 | val loss: 0.3492 | val MAE/class: [0.57, 0.51, 0.46, 0.40, 0.51, 0.73, 0.52, 0.67, 0.39, 0.67, 0.67, 0.47, 0.80]\n",
            "Epoch 02 | train loss: 0.3165 | val loss: 0.3433 | val MAE/class: [0.57, 0.51, 0.46, 0.41, 0.52, 0.73, 0.51, 0.68, 0.40, 0.67, 0.67, 0.47, 0.78]\n",
            "Training complete.  Best val loss: 0.3432713747024536\n"
          ]
        }
      ],
      "source": [
        "# -----------------  TRAINING DRIVER  -----------------\n",
        "EPOCHS           = 2\n",
        "UNFREEZE_EPOCH   = 5\n",
        "best_val_loss    = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    # unfreeze backbone once warm-up is done\n",
        "    if epoch == UNFREEZE_EPOCH + 1:\n",
        "        for name, p in model.named_parameters():\n",
        "            if not name.startswith(\"head.\"):\n",
        "                p.requires_grad_(True)\n",
        "        print(\"Backbone unfrozen ✅\")\n",
        "\n",
        "    train_loss = train_epoch(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_loss, val_mae = eval_epoch(test_loader, model, criterion)\n",
        "\n",
        "    # ---- logging ----\n",
        "    mae_str = \", \".join([f\"{m:.2f}\" for m in val_mae])\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss: {train_loss:.4f} | \"\n",
        "          f\"val loss: {val_loss:.4f} | \"\n",
        "          f\"val MAE/class: [{mae_str}]\")\n",
        "\n",
        "    # save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"best_choco_count.pt\")\n",
        "\n",
        "print(\"Training complete.  Best val loss:\", best_val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndM9O5ucbt_7"
      },
      "id": "ndM9O5ucbt_7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}